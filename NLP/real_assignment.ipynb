{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================KNN Result================\n",
      "KNN accuaracy(K = 1) :  0.6977611940298507     KNN f1(K = 1) :  0.6582278481012659\n",
      "KNN accuaracy(K = 2) :  0.7611940298507462     KNN f1(K = 2) :  0.6666666666666666\n",
      "KNN accuaracy(K = 3) :  0.8022388059701493     KNN f1(K = 3) :  0.7511737089201879\n",
      "KNN accuaracy(K = 4) :  0.8059701492537313     KNN f1(K = 4) :  0.7425742574257426\n",
      "KNN accuaracy(K = 5) :  0.8432835820895522     KNN f1(K = 5) :  0.8108108108108109\n",
      "\n",
      "\n",
      "============Logistic Regression============\n",
      "Logistic Regression accuracy(iter==20) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==40) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==60) : "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==80) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==100) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==100,C=1) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==100,C=2) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==100,C=3) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==100,C=4) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "Logistic Regression accuracy(iter==100,C=5) :  0.8134328358208955     Logistic Regression f1 :  0.7727272727272728\n",
      "\n",
      "\n",
      "==============Decision Tree==============\n",
      "Decision Tree accuracy :  0.8134328358208955     Decision Tree f1 :  0.766355140186916\n",
      "\n",
      "\n",
      "========Decision Tree with Bagging========\n",
      "Decision Tree with Bagging accuracy(n=1) :  0.8022388059701493 Decision Tree with Bagging f1(n=1) :  0.7439613526570049\n",
      "Decision Tree with Bagging accuracy(n=2) :  0.8171641791044776 Decision Tree with Bagging f1(n=2) :  0.7677725118483413\n",
      "Decision Tree with Bagging accuracy(n=3) :  0.8395522388059702 Decision Tree with Bagging f1(n=3) :  0.8054298642533937\n",
      "Decision Tree with Bagging accuracy(n=4) :  0.835820895522388 Decision Tree with Bagging f1(n=4) :  0.8035714285714286\n",
      "Decision Tree with Bagging accuracy(n=5) :  0.835820895522388 Decision Tree with Bagging f1(n=5) :  0.808695652173913\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "def bar_chart(feature):  #대략적인 생존자와 사망자 파악을 위해 차트 그래프 \n",
    "    Survived = train[train['Survived'] == 1][feature].value_counts()\n",
    "    Dead = train[train['Survived'] == 0][feature].value_counts()\n",
    "    df = pd.DataFrame([Survived,Dead])\n",
    "    df.index = ['Survived','Dead']\n",
    "    df.plot(kind ='bar', stacked = True)\n",
    "    \n",
    "    \n",
    "def dummy_process(data, columns):\n",
    "    for column in columns:\n",
    "        data = pd.concat([data, pd.get_dummies(data[column], prefix = column)], axis=1)\n",
    "        data = data.drop(column, axis=1)\n",
    "    return data\n",
    "    \n",
    "    \n",
    "for data in train: #나이에 대한 결측치가 많기 때문에 여러 feature중 나이를 어느정도 유추할 수 있는 근거로 이름을 사용\n",
    "    train['Name_part'] = train.Name.str.extract('([A-Za-z]+)\\.',expand = False)  \n",
    "\n",
    "\n",
    "    \n",
    "train['Name_part'] = train['Name_part'].replace(['Capt','Col'],'Minor')             \n",
    "train['Name_part'] = train['Name_part'].replace(['Ms','Lady'],'Miss')\n",
    "train['Name_part'] = train['Name_part'].replace(['Mme','Mlle','Countess'],'Mrs')\n",
    "train['Name_part'] = train['Name_part'].replace(['Rev','Jonkheer','Major','Sir','Don'],'Mr')    #소수의 이름은 다수 이름으로 편입시킴\n",
    "\n",
    "\n",
    "for N_partition,age in train.groupby('Name_part')['Age'].median().iteritems():\n",
    "    train.loc[(train['Name_part']==N_partition) & (train['Age'].isnull()), 'Age'] = age\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "train = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis = 1)  # 고려하지 않은 정보 삭제\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = train.fillna({\"Embarked\" : 'S'})  #다수가 S이기 때문에 S로 간주하고 데이터 처리\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embarked_number = {'S' : 1, 'Q' : 2, 'C' : 3}                      #embarked 넘버링으로 맵핑\n",
    "train['Embarked'] = train['Embarked'].map(embarked_number)\n",
    "test['Embarked'] = test['Embarked'].map(embarked_number)\n",
    "\n",
    "\n",
    "dummy_features = ['Sex','Pclass','Embarked']\n",
    "train = dummy_process(train, dummy_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train['Age'] = train['Age'].fillna(-0.5)\n",
    "\n",
    "\n",
    "bins = [-1, 0, 5, 13, 19, 60 , 200]                                                #결측치 마이너스로 넣어두고 나이별로 그룹화 맵핑한다.\n",
    "labels = ['UNK', 'Baby', 'Child', 'Teenager','Adult','Senior']\n",
    "train['Age_Group'] = pd.cut(train['Age'], bins, labels = labels)\n",
    "\n",
    "age_number = {'UNK':0, 'Baby' : 1,'Child' : 2, 'Teenager' : 3, 'Adult' : 4, 'Senior':5}\n",
    "train['Age_Group'] = train['Age_Group'].map(age_number)\n",
    "train = train.drop(['Age'], axis = 1)\n",
    "\n",
    "train['Fare'] = pd.qcut(train['Fare'], 5, labels = [1,2,3,4,5])  #Fare 값이 편차가 크긴 하지만 요금에 따른 생존 여부가 지대한 영향을 미친다는 근거가 부족해\n",
    "                                                                 #5 그룹으로 단순히 나누었다.\n",
    "    \n",
    "    \n",
    "train = train.drop('Name_part', axis =1 )\n",
    "\n",
    "train['Family'] = train['Parch']+train['SibSp']   # 두 특성은 같이 가족으로 고려해도 되기 때문에 family로 합치고 두 특성 삭제\n",
    "train = train.drop('SibSp',axis = 1)\n",
    "train = train.drop('Parch',axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "x = train.drop('Survived', axis =1)\n",
    "y = train['Survived']\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state = 40)   #train.csv 파일을 7:3으로 나누어서 테스트\n",
    "\n",
    "\n",
    "def train_model(model):\n",
    "    model.fit(x_train,y_train)\n",
    "    prediction = model.predict(x_test)\n",
    "    accuracy = accuracy_score(prediction,y_test)\n",
    "    return accuracy\n",
    "\n",
    "def train_model_f1 (model):\n",
    "    model.fit(x_train,y_train)\n",
    "    prediction = model.predict(x_test)\n",
    "    f1 = f1_score(prediction,y_test)\n",
    "    return f1\n",
    "\n",
    "\n",
    "train_for_log = train\n",
    "train_for_log = train_for_log.drop(['Fare','Age_Group','Family'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"================KNN Result================\")\n",
    "print(\"KNN accuaracy(K = 1) : \",train_model(KNeighborsClassifier(n_neighbors = 1)),\"    KNN f1(K = 1) : \",train_model_f1(KNeighborsClassifier(n_neighbors = 1)))\n",
    "print(\"KNN accuaracy(K = 2) : \",train_model(KNeighborsClassifier(n_neighbors = 2)),\"    KNN f1(K = 2) : \",train_model_f1(KNeighborsClassifier(n_neighbors = 2)))\n",
    "print(\"KNN accuaracy(K = 3) : \",train_model(KNeighborsClassifier(n_neighbors = 3)),\"    KNN f1(K = 3) : \",train_model_f1(KNeighborsClassifier(n_neighbors = 3)))\n",
    "print(\"KNN accuaracy(K = 4) : \",train_model(KNeighborsClassifier(n_neighbors = 4)),\"    KNN f1(K = 4) : \",train_model_f1(KNeighborsClassifier(n_neighbors = 4)))\n",
    "print(\"KNN accuaracy(K = 5) : \",train_model(KNeighborsClassifier(n_neighbors = 5)),\"    KNN f1(K = 5) : \",train_model_f1(KNeighborsClassifier(n_neighbors = 5)))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"============Logistic Regression============\")\n",
    "print(\"Logistic Regression accuracy(iter==20) : \" ,train_model(LogisticRegression(max_iter=20)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=20)))\n",
    "print(\"Logistic Regression accuracy(iter==40) : \", train_model(LogisticRegression(max_iter=40)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=40)))\n",
    "print(\"Logistic Regression accuracy(iter==60) : \", train_model(LogisticRegression(max_iter=60)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=60)))\n",
    "print(\"Logistic Regression accuracy(iter==80) : \", train_model(LogisticRegression(max_iter=80)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=80)))\n",
    "print(\"Logistic Regression accuracy(iter==100) : \", train_model(LogisticRegression(max_iter=100)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=100)))\n",
    "print(\"Logistic Regression accuracy(iter==100,C=1) : \", train_model(LogisticRegression(max_iter=100,C=1)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=100,C=1)))\n",
    "print(\"Logistic Regression accuracy(iter==100,C=2) : \", train_model(LogisticRegression(max_iter=100,C=2)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=100,C=2)))\n",
    "print(\"Logistic Regression accuracy(iter==100,C=3) : \", train_model(LogisticRegression(max_iter=100,C=3)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=100,C=3)))\n",
    "print(\"Logistic Regression accuracy(iter==100,C=4) : \", train_model(LogisticRegression(max_iter=100,C=4)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=100,C=4)))\n",
    "print(\"Logistic Regression accuracy(iter==100,C=5) : \", train_model(LogisticRegression(max_iter=100,C=5)),\"    Logistic Regression f1 : \",train_model_f1(LogisticRegression(max_iter=100,C=5)))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"==============Decision Tree==============\")\n",
    "print(\"Decision Tree accuracy : \",train_model(DecisionTreeClassifier(random_state=30)),\"    Decision Tree f1 : \",train_model_f1(DecisionTreeClassifier(random_state=30)))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "bag_model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=1)\n",
    "\n",
    "print(\"========Decision Tree with Bagging========\")\n",
    "print(\"Decision Tree with Bagging accuracy(n=1) : \",train_model(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=1)),\"Decision Tree with Bagging f1(n=1) : \",train_model_f1(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=1)))\n",
    "print(\"Decision Tree with Bagging accuracy(n=2) : \",train_model(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=2)),\"Decision Tree with Bagging f1(n=2) : \",train_model_f1(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=2)))\n",
    "print(\"Decision Tree with Bagging accuracy(n=3) : \",train_model(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=3)),\"Decision Tree with Bagging f1(n=3) : \",train_model_f1(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=3)))\n",
    "print(\"Decision Tree with Bagging accuracy(n=4) : \",train_model(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=4)),\"Decision Tree with Bagging f1(n=4) : \",train_model_f1(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=4)))\n",
    "print(\"Decision Tree with Bagging accuracy(n=5) : \",train_model(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=5)),\"Decision Tree with Bagging f1(n=5) : \",train_model_f1(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=30,n_estimators=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
